{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nickradunovic/ragdemo_politie/blob/main/rechtspraak_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Reh2ER2HCY3X"
      },
      "source": [
        "# Retrieval-Augmented Generation (RAG) with Public Justice Data\n",
        "\n",
        "# How RAG Works\n",
        "Traditional LLMs like GPT-4 come pre-trained on massive public datasets, allowing for incredible natural language processing capabilities out of the box. However, their utility is limited without access to your own private data. RAG tries to combat this limitation by introducing a retrieval mechanism that allows the model to access externally provided data sources like private data. Here's a simplified overview of how it works:\n",
        "\n",
        "- **Retrieval**: The model performs an initial retrieval step to gather contextually relevant information from a knowledge base or external documents.\n",
        "\n",
        "- **Augmentation**: The retrieved information is then seamlessly integrated into the generation process, augmenting the model's understanding and improving the quality of generated content.\n",
        "\n",
        "- **Generation**: The model generates text, now equipped with the additional knowledge obtained through retrieval, resulting in more informed and contextually rich output.\n",
        "\n",
        "In this demo, we will be working with [public case law data](https://uitspraken.rechtspraak.nl/resultaat?zoekterm=&inhoudsindicatie=&publicatiestatus=ps1&sort=Relevance&rechtsgebied=r3) containing records of lawsuit rulings of criminal law. We will demonstrate how RAG can be used to retrieve relevant records and generate accurate, context-aware responses to queries about this data. This will showcase the behind-the-scenes workings of RAG and illustrate how it can add significant value over standard LLM implementations that do not utilize a retrieval mechanism.\n",
        "\n",
        "## Objectives\n",
        "By the end of this tutorial, you will:\n",
        "\n",
        "- Understand the core concepts of RAG and its advantages over standard LLMs.\n",
        "\n",
        "- Learn how to implement a RAG system using public justice data.\n",
        "\n",
        "- See practical examples of querying the data and retrieving contextually relevant responses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNe2bnjiCrBq"
      },
      "source": [
        "# Create the LLM prompt with question and answer\n",
        "\n",
        "First, let's create an `.env` file containing the necessary OAI keys."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create .env file with OAI keys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install required dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile requirements.txt\n",
        "python-dotenv\n",
        "openai\n",
        "azure-search-documents\n",
        "azure-core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import Required Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import uuid\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from openai import AzureOpenAI\n",
        "from typing import List\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from azure.search.documents import SearchClient\n",
        "from azure.search.documents.models import VectorizedQuery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initiating the Azure SearchClient and AzureOpenAI instance\n",
        "\n",
        "In this first step, we will set up our environment and connect to the necessary Azure services. This involves loading environment variables and initializing clients for Azure Cognitive Search and Azure OpenAI. These connections are crucial as they allow us to interact with the search index and the OpenAI models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_9E-mraWx-N"
      },
      "outputs": [],
      "source": [
        "load_dotenv()\n",
        "\n",
        "credential = AzureKeyCredential(os.environ.get(\"SEARCH_KEY\"))\n",
        "search_client = SearchClient(\n",
        "    endpoint=os.environ.get(\"SEARCH_ENDPOINT\"),\n",
        "    index_name=os.environ.get(\"SEARCH_INDEX_NAME\"),\n",
        "    credential=credential,\n",
        ")\n",
        "client = AzureOpenAI(\n",
        "    api_key=os.getenv(\"OPENAI_KEY\"),\n",
        "    api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
        "    azure_endpoint=os.getenv(\"OPENAI_ENDPOINT\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implementing the Assistant Class\n",
        "\n",
        "In this step, we will implement the Assistant class, which serves as the core of our RAG system. This class will manage interactions with the Azure Cognitive Search and Azure OpenAI services. It includes methods for searching documents and generating responses to user queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OR_YG_5Wx-O"
      },
      "outputs": [],
      "source": [
        "class Assistant:\n",
        "    def __init__(self, search_client: SearchClient, openai_client, model: str = \"gpt-4o\"):\n",
        "        self.search_client = search_client\n",
        "        self.llm = openai_client.chat.completions.create\n",
        "        self.embedding = openai_client.embeddings.create\n",
        "        self.model = model\n",
        "\n",
        "    def search(self, query: str, top_n_documents: int) -> List:\n",
        "        \"\"\"\n",
        "        Searches for documents based on the given query.\n",
        "\n",
        "        Args:\n",
        "            query (str): The search query.\n",
        "            top_n_documents (int): Number of top documents to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of search results.\n",
        "        \"\"\"\n",
        "        query_embedding = (\n",
        "            self.embedding(input=[query], model=os.getenv(\"EMBEDDING_NAME\"))\n",
        "            .data[0]\n",
        "            .embedding\n",
        "        )\n",
        "\n",
        "        # Azure AI search requires a vector query\n",
        "        vector_query = VectorizedQuery(\n",
        "            vector=query_embedding,\n",
        "            k_nearest_neighbors=top_n_documents,\n",
        "            fields=\"content_vector\",\n",
        "            exhaustive=True,\n",
        "        )\n",
        "\n",
        "        # passing in query in search_text makes it 'hybrid' search\n",
        "        search_results = self.search_client.search(\n",
        "            search_text=query, vector_queries=[vector_query], top=top_n_documents\n",
        "        )\n",
        "\n",
        "        return search_results\n",
        "\n",
        "    def rag_chat(self, question: str) -> str:\n",
        "        \"\"\"\n",
        "        Ask a question to the RAG chatbot\n",
        "\n",
        "        Args:\n",
        "            question (str): The question to ask the chatbot.\n",
        "\n",
        "        Returns:\n",
        "            str: The response from the chatbot.\n",
        "        \"\"\"\n",
        "        search_results = self.search(question, top_n_documents=7)\n",
        "\n",
        "        documents_string = \"\"\n",
        "        for result in search_results:\n",
        "            documents_string += (\n",
        "                f\"ECLI: {result['title']} | Text: {result['content']}\\n\\n\"\n",
        "            )\n",
        "\n",
        "        response = self.llm(\n",
        "            model=self.model,\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"Je bent een assistent die vragen beantwoordt over Rechtspraken. \\\n",
        "                        Bij elke vraag krijg je relevante info van verschillende rechtszaken meegestuurd in CONTEXT. \\\n",
        "                        De vraag staat bij VRAAG. \\\n",
        "                        Je moet je antwoord enkel en alleen baseren op de meegestuurde info \\\n",
        "                        Refereer in je antwoord per stuk. \\\n",
        "                        VRAAG: {question}. CONTEXT:\\n{documents_string}. \\nJOUW ANTWOORD:\",\n",
        "                },\n",
        "            ],\n",
        "        )\n",
        "\n",
        "        return response.choices[0].message.content, documents_string\n",
        "\n",
        "    def normal_chat(self, question: str) -> str:\n",
        "        \"\"\"\n",
        "        Ask a question to the RAG chatbot\n",
        "\n",
        "        Args:\n",
        "            question (str): The question to ask the chatbot.\n",
        "\n",
        "        Returns:\n",
        "            str: The response from the chatbot.\n",
        "        \"\"\"\n",
        "        response = self.llm(\n",
        "            model=self.model,\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"Je bent een assistent die vragen beantwoordt over Rechtspraken.\",\n",
        "                },\n",
        "                {\"role\": \"user\", \"content\": f\"VRAAG: {question}\"},\n",
        "            ],\n",
        "        )\n",
        "\n",
        "        return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Instantiating the Assistant Class\n",
        "Now that we have defined the Assistant class, the next step is to create an instance of this class. This will allow us to use the methods we defined to perform searches and generate responses. By passing in the search_client and client (initialized in the previous step) to the Assistant constructor, we can set up our assistant for use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyxFGHXoWx-O"
      },
      "outputs": [],
      "source": [
        "assistant = Assistant(search_client, client)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating a Response Using the RAG System\n",
        "Now, we will utilize the Assistant class instance to generate a response to a specific questions related and not related to legal cases. See whether or not there is a difference in response quality between quantitative and substantive/content question. Try out your own questions as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbUW1yE7Wx-O"
      },
      "outputs": [],
      "source": [
        "vraag = \"welke rechtszaken hebben een man als dader?\"\n",
        "rag_antwoord, rag_documents = assistant.rag_chat(vraag)\n",
        "print(rag_antwoord)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5N5kWtIWWx-P",
        "outputId": "c3faa16d-90ad-4049-c4f0-bcabc7e1c737"
      },
      "outputs": [],
      "source": [
        "vraag = \"in welke rechtszaken komt het gebruik van een vuurwapen voor? Het vuurwapen hoeft niet te zijn afgegaan.\"\n",
        "rag_antwoord, rag_documents = assistant.rag_chat(vraag)\n",
        "print(rag_antwoord)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vraag = \"In welke rechtzaken zijn kinderen betrokken?\"\n",
        "rag_antwoord, rag_documents = assistant.rag_chat(vraag)\n",
        "print(rag_antwoord)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Printing the CONTEXT of the prompt to see what documents were retrieved and used for its response\n",
        "\n",
        "It is possible to print the context of the prompt used by the RAG assistent. Note, that the method `rag_chat` also returns the retrieved documents. Print these documents to see the result of the vector search. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(rag_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How to handle unrelated questions?\n",
        "\n",
        "At the moment, the RAG Assistant allows users to ask questions unrelated to the data in the vector database. Imagine a use case where this behaviour is not acceptable. How can we ensure that the RAG assistent gives appropiate responses and answers questions only related to the use case?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vraag = \"Hoe kan het zijn dat appels verschillende kleuren hebben (bijvoorbeeld rood, groen, geel of een combinatie van deze kleuren)?\"\n",
        "rag_antwoord, rag_documents = assistant.rag_chat(vraag)\n",
        "print(rag_antwoord)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**TASK**: Reprogram the RAG Assistant so that it does not answer questions unrelated to legal cases.\n",
        "\n",
        "Does it work? Discuss why it does / why it doesn't."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write a query unrelated to legal case law data to test whether or not the RAG Assistent answers this kind of questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Utilize the 'normal chat' that doesn't incorporate RAG functionality\n",
        "\n",
        "If you would like to ask questions to the OAI model without RAG functionality, make an OAI call using the `normal_chat` method of the Assistant. Using this non-RAG chat one can play around and try to spot differences between a LLM with RAG functionality and one without. \n",
        "\n",
        "**TASK**: What are the use cases and down- and upsides of using RAG compared to using a regular LLM?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instead of using `rag_chat`, write queries using the `normal_chat` method of the Assistant instead. Try out various queries and compare with the `rag_chat` method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chunk size\n",
        "\n",
        "Chunk size is a parameter that determines the length of each segment (or \"chunk\") of text that the source documents are divided into. In our implementation, the chunk size is specified in terms of the number of characters. The embeddings in the vector store we just used where created using a chunk size of `20480`, which is large enough for each legal case. However, what would happen if we use a smaller chunk size like `1024`? Now we use the search_index_name `SEARCH_INDEX_NAME=index-politiedemo` that was made with a large (`20480`) chunk size.\n",
        "\n",
        "We prepared some indexes in our vector store for you to play with. Change the search_index_name to `SEARCH_INDEX_NAME=index-politiedemo-s` so we can see the effect of having a smaller chunk size of `1024`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "search_client_small = SearchClient(\n",
        "    endpoint=os.environ.get(\"SEARCH_ENDPOINT\"),\n",
        "    index_name=\"index-politiedemo-s\",\n",
        "    credential=credential,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "assistant_small_chunk_size = Assistant(search_client_small, client)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vraag = \"welke rechtszaken hebben een man als dader?\"\n",
        "rag_antwoord, rag_documents = assistant_small_chunk_size.rag_chat(vraag)\n",
        "print(rag_antwoord)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Changing the `top_n_documents`\n",
        "\n",
        "By using a smaller chunk size, it might be possible to retrieve a larger number of documents from the vector database. At the moment, `top_n_documents = 7`. Try retrieving a larger number of documents per query as see how this impacts the response quality. \n",
        "\n",
        "**NOTE**: make sure to use the index created using the smaller chunk size of `1024`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# write a query for the RAG Assistant while retrieving more than 7 documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Changing the OAI model\n",
        "\n",
        "There are different LLMs one can use for their RAG-implementation as well. We just used OpenAI GPT-4o in the RAG-assistant we just played with. However, you can imagine use cases and projects that opt to using a model not from OpenAI. For this demo, we keep on using OpenAI but we can play with different models they provide.\n",
        "\n",
        "Next to `gpt-4o`, we also deployed `gpt-35-turbo` and `gpt-4` for this demo. Each model has a different context window size. Below we try to use `gpt-35-turbo` with the big chunk size search_client. Note that this does not work.\n",
        "\n",
        "**TASK**: Change the model or the search client in order to make it work.\n",
        "Even though the context window size is smaller for `gpt-35-turbo` compared to the other models, it might still be preferred over the others. What would be reasons to opt for one LLM over the others?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "assistant_gpt35_turbo = Assistant(search_client_small, client, model=\"gpt-35-turbo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try out the new RAG-assistant to see if you can find any differences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vector search is language independent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vraag = \"Which person was involved in multiple cases\"\n",
        "rag_antwoord, rag_documents = assistant.rag_chat(vraag)\n",
        "print(rag_antwoord)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Things to consider and try out for yourself:\n",
        "- How can we utilize system prompt to get the response that we want in our application (can we include any guard rails?)\n",
        "- What could be ways to enforce that the RAG Assistant is consistent in its responses?\n",
        "- Difference in quality of response between quantitative questions (how many ...?, what is the greatest ...?) and qualitative (inhoudelijke) questions. \n",
        "- What could be shortcomings of this RAG implementation? --> (1) no chat history is saved at the moment, (2) ..., ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try out your own queries after playing with one of the points mentioned above."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "politie_rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
