{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nickradunovic/ragdemo_politie/blob/main/rechtspraak_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Reh2ER2HCY3X"
      },
      "source": [
        "# Retrieval Augmented Generation in LLMs\n",
        "\n",
        "[Introductory text]\n",
        "\n",
        "\n",
        "## CHAPTER 1: Creating the embedded model and vector database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irWJL0kwCFv4"
      },
      "outputs": [],
      "source": [
        "# First, we make sure to install the required dependencies.\n",
        "!pip install python-dotenv\n",
        "!pip install openai\n",
        "!pip install azure-search-documents\n",
        "!pip install azure-core"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSfpunGACTcs"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNe2bnjiCrBq"
      },
      "source": [
        "# CHAPTER 2: Create the LLM prompt with question and answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_9E-mraWx-N"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from openai import AzureOpenAI\n",
        "from typing import List\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from azure.search.documents import SearchClient\n",
        "from azure.search.documents.models import VectorizedQuery\n",
        "from google.colab import userdata\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "credential = AzureKeyCredential(os.environ.get(\"SEARCH_KEY\"))\n",
        "search_client = SearchClient(\n",
        "    endpoint=os.environ.get(\"SEARCH_ENDPOINT\"),\n",
        "    index_name=os.environ.get(\"SEARCH_INDEX_NAME\"),\n",
        "    credential=credential,\n",
        ")\n",
        "client = AzureOpenAI(\n",
        "    api_key=os.getenv(\"OPENAI_KEY\"),\n",
        "    api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
        "    azure_endpoint=os.getenv(\"OPENAI_ENDPOINT\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OR_YG_5Wx-O"
      },
      "outputs": [],
      "source": [
        "class Assistant:\n",
        "    def __init__(self, search_client: SearchClient, openai_client):\n",
        "        self.search_client = search_client\n",
        "        self.llm = openai_client.chat.completions.create\n",
        "        self.embedding = openai_client.embeddings.create\n",
        "\n",
        "    def search(self, query: str, top_n_documents: int) -> List:\n",
        "        \"\"\"\n",
        "        Searches for documents based on the given query.\n",
        "\n",
        "        Args:\n",
        "            query (str): The search query.\n",
        "            top_n_documents (int): Number of top documents to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of search results.\n",
        "        \"\"\"\n",
        "        query_embedding = (\n",
        "            self.embedding(input=[query], model=userdata.get(\"EMBEDDING_NAME\"))\n",
        "            .data[0]\n",
        "            .embedding\n",
        "        )\n",
        "\n",
        "        # Azure AI search requires a vector query\n",
        "        vector_query = VectorizedQuery(\n",
        "            vector=query_embedding,\n",
        "            k_nearest_neighbors=top_n_documents,\n",
        "            fields=\"content_vector\",\n",
        "            exhaustive=True,\n",
        "        )\n",
        "\n",
        "        # passing in query in search_text makes it 'hybrid' search\n",
        "        search_results = self.search_client.search(\n",
        "            search_text=query, vector_queries=[vector_query], top=top_n_documents\n",
        "        )\n",
        "\n",
        "        return search_results\n",
        "\n",
        "    def rag_chat(self, question: str) -> str:\n",
        "        \"\"\"\n",
        "        Ask a question to the RAG chatbot\n",
        "\n",
        "        Args:\n",
        "            question (str): The question to ask the chatbot.\n",
        "\n",
        "        Returns:\n",
        "            str: The response from the chatbot.\n",
        "        \"\"\"\n",
        "        search_results = self.search(question, top_n_documents=7)\n",
        "\n",
        "        documents_string = \"\"\n",
        "        for result in search_results:\n",
        "            documents_string += (\n",
        "                f\"ECLI: {result['title']} | Text: {result['content']}\\n\\n\"\n",
        "            )\n",
        "\n",
        "        response = self.llm(\n",
        "            model=\"gpt-35-turbo\",  # gpt-35-turbo\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"Je bent een assistent die vragen beantwoordt over Rechtspraken. \\\n",
        "                        Bij elke vraag krijg je relevante info van verschillende rechtszaken meegestuurd in CONTEXT. \\\n",
        "                        De vraag staat bij VRAAG. \\\n",
        "                        Je moet je antwoord enkel en alleen baseren op de meegestuurde info \\\n",
        "                        Refereer in je antwoord per stuk. \\\n",
        "                        VRAAG: {question}. CONTEXT:\\n{documents_string}. \\nJOUW ANTWOORD:\",\n",
        "                },\n",
        "            ],\n",
        "        )\n",
        "\n",
        "        return response.choices[0].message.content, documents_string\n",
        "\n",
        "    def normal_chat(self, question: str) -> str:\n",
        "        \"\"\"\n",
        "        Ask a question to the RAG chatbot\n",
        "\n",
        "        Args:\n",
        "            question (str): The question to ask the chatbot.\n",
        "\n",
        "        Returns:\n",
        "            str: The response from the chatbot.\n",
        "        \"\"\"\n",
        "        response = self.llm(\n",
        "            model=\"gpt-35-turbo\",  # gpt-35-turbo\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"Je bent een assistent die vragen beantwoordt over Rechtspraken.\",\n",
        "                },\n",
        "                {\"role\": \"user\", \"content\": f\"VRAAG: {question}\"},\n",
        "            ],\n",
        "        )\n",
        "\n",
        "        return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyxFGHXoWx-O"
      },
      "outputs": [],
      "source": [
        "assistant = Assistant(search_client, client)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbUW1yE7Wx-O"
      },
      "outputs": [],
      "source": [
        "# vraag = \"welke rechtszaken hebben een man als dader\"\n",
        "# vraag = \"in welke rechtszaken komt het gebruik van een vuurwapen voor?\"\n",
        "vraag = \"In welke rechtzaken zijn kinderen betrokken? Benoem ook de id van de rechtzaak\"\n",
        "rag_antwoord, rag_documents = assistant.rag_chat(vraag)\n",
        "antwoord, documents = assistant.rag_chat(vraag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5N5kWtIWWx-P",
        "outputId": "c3faa16d-90ad-4049-c4f0-bcabc7e1c737"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In de volgende rechtszaken zijn kinderen betrokken:\n",
            "\n",
            "- ECLI:NL:RBNHO:2021:9613: In deze zaak zijn twee minderjarigen [kind 1] en [kind 2] betrokken. Ze zijn door de man erkend en hebben hun hoofdverblijfplaats bij de vrouw. Partijen zijn gezamenlijk belast met het gezag over de minderjarigen.\n",
            "\n",
            "- ECLI:NL:RBROT:2021:10072: In deze zaak zijn drie minderjarigen, genaamd [naam kind 1], [naam kind 2] en [naam kind 3], betrokken. De kinderrechter acht een verlenging van de ondertoezichtstelling voor de duur van negen maanden noodzakelijk. Het is van belang dat op een zo kort mogelijke termijn duidelijkheid komt over het hoofdverblijf van de kinderen en dat afspraken over de kinderalimentatie worden vastgelegd. Met name is het van belang dat een omgangsregeling tussen de moeder en de kinderen wordt vormgegeven - op een manier die voor beide ouders haalbaar zal zijn en die aansluit op de behoeften van de kinderen.\n",
            "\n",
            "- ECLI:NL:GHARL:2020:219: In deze zaak zijn [de minderjarige1] en [de minderjarige2] betrokken, kinderen van de vader en de moeder. Het ging hier om een geschil over het gezag.\n",
            "\n",
            "- ECLI:NL:RBNHO:2022:1506: In deze zaak gaat het om [de minderjarige1], waarvan het gezag van de moeder is beÃ«indigd en de GI tot voogd is benoemd. \n",
            "\n",
            "- ECLI:NL:GHARL:2022:10831: In deze zaak is [de minderjarige1] betrokken. De kinderrechter heeft de GI gemachtigd [de minderjarige1] uit huis te plaatsen in een voorziening voor pleegzorg.\n",
            "\n",
            "- ECLI:NL:RBDHA:2021:467: Deze zaak ging om een internationale kinderontvoering en betrokken [minderjarige 1] en [minderjarige 2]. Er is een bijzondere curator benoemd die onder meer heeft onderzocht wat [minderjarige 1] en [minderjarige 2] zelf aangeven over een eventueel verblijf in Groot-BrittanniÃ« en een eventueel verblijf in Nederland. Beide kinderen zijn ook gehoord door de meervoudige kamer.\n"
          ]
        }
      ],
      "source": [
        "print(rag_antwoord)\n",
        "# print(rag_documents)\n",
        "# print(antwoord)\n",
        "# print(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4cyK-8pWx-P"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "politie_rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
